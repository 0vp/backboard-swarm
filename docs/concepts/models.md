> ## Documentation Index
> Fetch the complete documentation index at: https://backboard-docs.docsalot.dev/llms.txt
> Use this file to discover all available pages before exploring further.


**Models** are the foundation of your AI interactions in Backboard. They represent different AI language models and embedding models available from various providers like OpenAI, Anthropic, Google, and more.

## Model Types

Backboard supports two primary types of models:

<CardGroup cols={2}>
  <Card title="Language Models" icon="brain">
    For text generation, chat completions, and reasoning tasks
  </Card>
  <Card title="Embedding Models" icon="vector-square">
    For converting text into vector representations for RAG and semantic search
  </Card>
</CardGroup>

## Key Concepts

### Providers
Model providers are the companies or services that offer AI models. Each provider has different models with varying capabilities, pricing, and performance characteristics.

Common providers include:
- **OpenAI** - GPT-4, GPT-5.2, text-embedding models
- **Anthropic** - Claude 3 series
- **Google** - Gemini models
- **Cohere** - Command models and embeddings
- **Openrouter** - Frontier models and Open-source models
- **Cerebras** - Fast inference Open-source models
- **Featherless** - Open-source models

### Model Names
Each model has a unique identifier that follows the format: `provider/model-name`. For example:
- `openai/gpt-4-turbo-preview`
- `anthropic/claude-3-opus`
- `google/gemini-pro`

### Model Pricing
Each model has separate pricing for input and output tokens, allowing you to optimize costs based on your usage patterns:

<Info>
Models now return detailed pricing information including separate costs for input and output tokens. This helps you accurately estimate and optimize your API usage costs.
</Info>

- **Input tokens**: Tokens sent to the model (your prompts, context, etc.)
- **Output tokens**: Tokens generated by the model (completions, responses)
- Pricing is shown per 1 million tokens in USD

## Using Model Endpoints

### Listing Available Models

Get all available language models with pricing information:

```python
import requests

response = requests.get(
    "https://app.backboard.io/api/models",
    headers={"X-API-Key": "your_api_key"}
)
models_data = response.json()

# Access the models with pricing
for model in models_data["models"]:
    print(f"Model: {model['provider']}/{model['name']}")
    print(f"  Context: {model['context_limit']} tokens")
    print(f"  Input cost: ${model.get('input_cost_per_1m_tokens', 'N/A')}/M")
    print(f"  Output cost: ${model.get('output_cost_per_1m_tokens', 'N/A')}/M")

# Filter by capability
chat_models = [m for m in models_data["models"] if m.get("supports_tools")]
```

### Listing Providers

See all available model providers:

```python
response = requests.get(
    "https://app.backboard.io/api/models/providers",
    headers={"X-API-Key": "your_api_key"}
)
providers = response.json()
```

### Getting Model Details

Retrieve specific information about a model, including detailed pricing:

```python
model_name = "openai/gpt-4-turbo-preview"
response = requests.get(
    f"https://app.backboard.io/api/models/{model_name}",
    headers={"X-API-Key": "your_api_key"}
)
model_info = response.json()

print(f"Context window: {model_info['context_limit']}")
print(f"Max output tokens: {model_info['max_output_tokens']}")
print(f"Input pricing: ${model_info['input_cost_per_1m_tokens']}/M tokens")
print(f"Output pricing: ${model_info['output_cost_per_1m_tokens']}/M tokens")
print(f"Supports tools: {model_info['supports_tools']}")
```

## Embedding Models

Embedding models convert text into numerical vectors for semantic operations:

### Listing Embedding Models

```python
response = requests.get(
    "https://app.backboard.io/api/models/embedding",
    headers={"X-API-Key": "your_api_key"}
)
embedding_models = response.json()
```

### Getting Embedding Model Details

```python
model_name = "openai/text-embedding-3-large"
response = requests.get(
    f"https://app.backboard.io/api/models/embedding/{model_name}",
    headers={"X-API-Key": "your_api_key"}
)
embedding_info = response.json()

print(f"Dimensions: {embedding_info['dimensions']}")
print(f"Max input tokens: {embedding_info['max_input_tokens']}")
```

## Model Configuration in Assistants

When creating an assistant, you specify which model to use for different operations:

```python
{
    "name": "My Assistant",
    "embedding_provider": "openai",     
    "embedding_model_name": "text-embedding-3-large",
    "embedding_dims": 3072
}
```

<Warning>
Embedding models cannot be changed after an assistant is created. Choose carefully based on your requirements for vector dimensions and performance.
</Warning>


## Example: Model Comparison with Cost Analysis

```python
import requests
import time

def get_model_pricing(provider, model_name):
    """Fetch current pricing for a specific model"""
    response = requests.get(
        f"https://app.backboard.io/api/models/{model_name}",
        headers={"X-API-Key": "your_api_key"}
    )
    return response.json()

def calculate_cost(usage, pricing_info):
    """Calculate cost based on token usage and pricing"""
    input_cost = (usage.get("input_tokens", 0) / 1_000_000) * pricing_info.get("input_cost_per_1m_tokens", 0)
    output_cost = (usage.get("output_tokens", 0) / 1_000_000) * pricing_info.get("output_cost_per_1m_tokens", 0)
    return {
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": input_cost + output_cost
    }

def compare_models(prompt, models):
    results = {}

    for model in models:
        # Get current pricing
        pricing_info = get_model_pricing(model["provider"], model["name"])

        start = time.time()

        response = requests.post(
            "https://app.backboard.io/api/threads/{thread_id}/messages",
            headers={"X-API-Key": "your_api_key"},
            data={
                "content": prompt,
                "model_provider": model["provider"],
                "model_name": model["name"],
                "stream": "false"
            }
        )

        elapsed = time.time() - start
        message_data = response.json()

        # Calculate costs
        usage = {
            "input_tokens": message_data.get("input_tokens", 0),
            "output_tokens": message_data.get("output_tokens", 0)
        }
        costs = calculate_cost(usage, pricing_info)

        results[model["name"]] = {
            "response": message_data,
            "latency": elapsed,
            "usage": usage,
            "costs": costs,
            "pricing_info": {
                "input_per_1m": pricing_info.get("input_cost_per_1m_tokens"),
                "output_per_1m": pricing_info.get("output_cost_per_1m_tokens")
            }
        }

    return results

# Compare different models
models_to_test = [
    {"provider": "openai", "name": "gpt-3.5-turbo"},
    {"provider": "openai", "name": "gpt-4-turbo-preview"},
    {"provider": "anthropic", "name": "claude-3-haiku"}
]

comparison = compare_models("Explain quantum computing", models_to_test)

# Display cost comparison
for model_name, data in comparison.items():
    print(f"\n{model_name}:")
    print(f"  Input tokens: {data['usage']['input_tokens']}")
    print(f"  Output tokens: {data['usage']['output_tokens']}")
    print(f"  Input cost: ${data['costs']['input_cost']:.6f}")
    print(f"  Output cost: ${data['costs']['output_cost']:.6f}")
    print(f"  Total cost: ${data['costs']['total_cost']:.6f}")
    print(f"  Latency: {data['latency']:.2f}s")
```

## Related Endpoints

- [List Models](/api-reference/models/list)
- [Get Model Details](/api-reference/models/get)
- [List Providers](/api-reference/models/list-providers)
- [List Embedding Models](/api-reference/models/list-embedding)
- [Get Embedding Model](/api-reference/models/get-embedding)